{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\n@dataclass\nclass BertConfig:\n    block_size : int = 512\n    vocab_size : int = 30_528 #a much better number than 30_522\n    n_layer : int = 12 \n    n_embd : int = 768\n    n_head : int = 12\n    intermediate_size : int = 3072\n    type_vocab_size :int = 2    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-08T13:47:44.844885Z","iopub.execute_input":"2024-08-08T13:47:44.845694Z","iopub.status.idle":"2024-08-08T13:47:48.724743Z","shell.execute_reply.started":"2024-08-08T13:47:44.845658Z","shell.execute_reply":"2024-08-08T13:47:48.723355Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class BertEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n        self.layernorm = nn.LayerNorm(config.n_embd, eps = 1e-12)\n        self.dropout = nn.Dropout(0.1)\n    \n    def forward(self, input_ids):\n        position_ids = torch.arange(config.block_size, device = input_ids.device)\n        input_embeds = self.word_embeddings(input_embeds) #(B, T, C)\n        position_embeds = self.positional_embeddings(position_ids) #(T, C)\n        embeddings = input_embeds + position_embeds #(B, T, C)\n        embeddings = self.layernorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings #(B, T, C)\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        self.key = nn.Linear(config.n_embd, config.n_embd)\n        self.query = nn.Linear(config.n_embd, config.n_embd)\n        self.value = nn.Linear(config.n_embd, config.n_embd)\n        self.n_head = config.n_head\n        self.dropout = nn.Dropout(0.1)\n        self.dense = nn.Linear(config.n_embd, config.n_embd)\n        self.layernorm = nn.LayerNorm(config.n_embd, eps=1e-12)\n\n    \n    def forward(self, x):\n        B, T, C = x.shape\n        key = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) #(B, nh, T, hs)\n        query = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) #(B, nh, T, hs)\n        value = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) #(B, nh, T, hs)\n        \n        wei = key @ query.transpose(-1, -2) / math.sqrt(C // self.n_head) #(B, nh, T, T)\n        attention_probs = F.softmax(wei, dim = -1)\n        attention_probs = self.dropout(attention_probs)\n        output = attention_probs @ value #(B, nh, T, hs)\n        output = output.transpose(1, 2) #(B, T, nh, hs)\n        output = output.view(B, T, C).contiguous() #(B, T, C)\n        output = self.dense(output) #(B, T, C)\n        output = self.layernorm(output + x) #(B, T, C)\n        output = self.dropout(output)\n        return output #(B, T, C)\n    \nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = nn.Sequential(\n            nn.Linear(config.n_embd, config.intermediate_size),\n            nn.GELU(approximation = 'tanh'),\n        )\n        self.output = nn.Sequential(\n            nn.Linear(config.intermediate_size, config.n_embd),\n            nn.Dropout(0.1),\n            #extracting the layernorm from here to add the residual \n        )\n        self.output_layernorm = nn.LayerNorm(config.n_embd, eps = 1e-12)\n        self.chunk_size_ffwd = 256\n        self.seq_len_dim = 1 \n    \n    def _feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output) #(B, T, intermediate_size)\n        layer_output = self.output(intermediate_output) #(B, T, C)\n        layer_output = self.output_layernorm(layer_output + attention_output)\n        return layer_output #(B, T, C)\n    \n    def apply_chunk_to_ffwd(self, forward_fn, chunk_size, chunk_dim, attn_out):\n        tensor_shape = attn_out.shape[chunk_dim]\n        assert tensor_shape % chunk_size == 0 \n        n_chunks = tensor_shape // chunk_size\n        attn_out_chunk = attn_out.chunk(n_chunks, dim = chunk_dim) #this is a tuple of the chunks\n        output_chunk = tuple(forward_fn(chunk) for chunk in attn_out_chunk)\n        return torch.cat(output_chunk, dim = chunk_dim)\n        \n    \n    def forward(self, x):\n        attention_output = self.attention(x)\n        layer_output = self.apply_chunk_to_ffwd(self._feed_forward_chunk, self.chunk_size_ffwd, self.seq_len_dim, attention_output)\n        return layer_output #(B, T, C)\n    \n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.n_layer)])\n     \n    def forward(self, x):\n        for layer in self.layer:\n            x = layer(x)\n        \n        return x #(B, T, C)\n    \nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.n_embd, config.n_embd)\n        self.activation = nn.Tanh()\n    \n    def forward(self, x):\n        first_token_tensor = x[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output #(B, T, C)\n    \nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.seq_relationship = nn.Linear(config.n_embd, 2)\n    \n    def forward(self, pooled_output):\n        return self.seq_relationship(pooled_output) #(B, T, 2)\n    \nclass BertModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embedding = BertEmbedding(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = 0.01)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        \n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = 0.01)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        \n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids):\n        B, T = input_ids.size()\n        device = input_ids.device\n        embedding_output = self.embedding(input_ids) #(B, T, C)\n        encoder_output = self.encoder(embedding_output) #(B, T, C)\n        pooled_output = self.pooler(encoder_output) #(B, T, C)\n        return pooled_output #(B, T, C)\n    \nclass BertForNextSentencePrediction(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n    \n    def forward(self, input_ids, labels):\n        pooled_output = self.bert(input_ids)\n        seq_relationship_score = self.cls(pooled_output)\n        next_sentence_loss = nn.CrossEntropyLoss(seq_relationship_score.view(-1, 2), labels.view(-1))\n        return (pooled_output, next_sentence_loss)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-07T14:42:17.564047Z","iopub.execute_input":"2024-08-07T14:42:17.565219Z","iopub.status.idle":"2024-08-07T14:42:17.587955Z","shell.execute_reply.started":"2024-08-07T14:42:17.565178Z","shell.execute_reply":"2024-08-07T14:42:17.586678Z"},"trusted":true},"execution_count":3,"outputs":[]}]}